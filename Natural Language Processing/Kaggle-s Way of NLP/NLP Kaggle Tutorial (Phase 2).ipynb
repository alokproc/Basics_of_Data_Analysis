{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Kaggle Tutorial (Phase 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [actual tutorial](https://www.kaggle.com/c/word2vec-nlp-tutorial#part-1-for-beginners-bag-of-words) was developed by **Angela Chapman** during her summer 2014 internship at Kaggle.  \n",
    "Full credits to her!\n",
    "\n",
    "This notebook by Prashant Brahmbhatt                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Distributed Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec** does not need labels in order to create meaningful representations.  \n",
    "This is useful, since most data in the real world is unlabeled. If the network is given enough training data (tens of billions of words), it produces word vectors with intriguing characteristics.  \n",
    "Words with similar meanings appear in clusters, and clusters are spaced such that some word relationships, such as analogies, can be reproduced using vector math.   \n",
    "**Example**  \n",
    "\"king - man + woman = queen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out [Google's code, writeup, and the accompanying papers](https://code.google.com/archive/p/word2vec/). [This presentation](https://docs.google.com/file/d/0B7XkCwpI5KDYRWRnd1RzWXQ2TWc/edit) is also helpful. The original code is in C, but it has since been ported to other languages, including Python.  \n",
    "Recent work out of Stanford has also applied deep learning to sentiment analysis; their code is available in Java. However, their approach, which relies on sentence parsing, cannot be applied in a straightforward way to paragraphs of arbitrary length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read in the data with pandas, we now use data with 50,000 additional reviews with no labels. \n",
    "When we built the **Bag of Words** model previously extra unlabeled training reviews were not useful.  \n",
    "However, since **Word2Vec** can learn from unlabeled data, these extra 50,000 reviews can now be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('kaggleNLP/labeledTrainData.tsv',sep = '\\t' , quoting = 3)\n",
    "test = pd.read_csv('kaggleNLP/testData.tsv', sep = '\\t', quoting = 3)\n",
    "unlabeled = pd.read_csv('kaggleNLP/unlabeledTrainData.tsv', sep = '\\t', quoting = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled Train reviews 25000\n",
      "Labeled Test reviews 25000\n",
      "Unlabeled reviews 50000\n"
     ]
    }
   ],
   "source": [
    "print(\"Labeled Train reviews {}\\nLabeled Test reviews {}\\nUnlabeled reviews {}\".\n",
    "      format(train['review'].size, test['review'].size, unlabeled['review'].size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, to train **Word2Vec** it is better not to remove **stop words** because the algorithm relies on the broader context of the sentence in order to produce high-quality word vectors.  \n",
    "For this reason, we will make stop word removal optional in the functions below. It also might be better not to remove numbers, but its up to the programmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_wordlist(review, remove_stopwords = False):\n",
    "    # Function to convert a document to a sequence of words\n",
    "    \n",
    "    #Removing HTML\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    \n",
    "    #Removing Non Letters\n",
    "    review_text = re.sub('[^a-zA-Z]',' ',review_text)\n",
    "    \n",
    "    #Convert words to lowercase and split\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    #Optionally removing stopwords\n",
    "    if remove_stopwords:\n",
    "        words = [w for w in words if w not in set(stopwords.words('english'))]\n",
    "    #Returing the list of words\n",
    "    return (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a specific input format. **Word2Vec** expects single sentences, each one as a list of words. In other words, the input format is a **list of lists**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spltting the paragraph is not very strightforward. There are all kinds of gotchas in natural language. English sentences can end with **\"?\"**, **\"!\"**, **\"\"\"**, or **\".\"**, among other things, and spacing and capitalization are not reliable guides either.  \n",
    "For this reason, we use **NLTK's punkt tokenizer** for sentence splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a function to split a review into parsed sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_sentences(review , tokenizer, remove_stopwords = False):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "\n",
    "    # Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "\n",
    "    #Looping over each sentence\n",
    "    sentences = []\n",
    "\n",
    "    #If sentence is empty skip otherwie call review_to_wordlist\n",
    "    for sentence in raw_sentences:\n",
    "        if len(sentence) >0:\n",
    "            sentences.append(review_to_wordlist(sentence, remove_stopwords))\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can apply this function to prepare our data for input to **Word2Vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from Reviews\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prash\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\prash\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:273: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\prash\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:336: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences for unlabeled data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prash\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:336: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\prash\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:336: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\prash\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:273: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\prash\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:273: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\prash\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:336: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\prash\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:273: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\prash\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:336: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "C:\\Users\\prash\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:273: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\prash\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\bs4\\__init__.py:336: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "print('Parsing sentences from Reviews')\n",
    "for rev in train['review']:\n",
    "    sentences += review_to_sentences(rev, tokenizer)\n",
    "    \n",
    "print('Parsing sentences for unlabeled data')\n",
    "for rev in unlabeled['review']:\n",
    "    sentences += review_to_sentences(rev, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have got warnings which are about the URLs in the sentences which are not a big trouble. Although we may want to remove these too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unconfirmed** (don't Read)\n",
    "\n",
    "A minor detail to note is the difference between the **\"+=\"** and **\"append\"** when it comes to Python lists. \n",
    "In many applications the two are interchangeable, but here they are not. \n",
    "When appending a list of lists to another list of lists, **\"append\"** will only append the first list; you need to use **\"+=\"** in order to join all of the lists at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Saving the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of parameter choices that affect the run time and the quality of the final model that is produced.  \n",
    "For details on the algorithms below, see the word2vec [API documentation](radimrehurek.com/gensim/models/word2vec.html) as well as the [Google documentation](https://code.google.com/archive/p/word2vec/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Architecture**: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results.\n",
    "\n",
    "\n",
    "- **Training algorithm**: Hierarchical softmax (default) or negative sampling. For us, the default worked well. \n",
    "\n",
    "\n",
    "- **Downsampling of frequent words**: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n",
    "\n",
    "\n",
    "- **Word vector dimensionality**: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300.\n",
    "\n",
    "\n",
    "- **Context / window size**: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n",
    "\n",
    "\n",
    "- **Worker threads**: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.\n",
    "\n",
    "\n",
    "- **Minimum word count**: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
